{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "As part of the** Amazon ML challenge**  2023, I along with my team developed a machine learning model to predict product length from catalog metadata. The objective of the challenge was to facilitate efficient packaging and storage of products in the warehouse.\n",
        "\n",
        "The training and testing data consisted of 2.2 million products, each with a unique product ID, title, description, bullet points, product type ID, and product length. My task was to build a model that could accurately predict the product length using these metadata features, despite the presence of noise in the data.\n",
        "\n",
        "To evaluate the performance of my model, we used the mean root mean square error, and the score was calculated as r square.\n",
        "For submission, we created a .csv file with the index set as the product ID and the target variable as the predicted product length. The submission file had to be of size 734736 x 2 and contain the correct index values and column names as provided in the sample submission file.\n",
        "\n",
        "Link for hakcthon\n",
        "https://www.hackerearth.com/challenges/competitive/amazon-ml-challenge-2023/machine-learning/product-length-prediction-7-85b7ef50/\n",
        "\n",
        "Overall, this challenge provided me with the opportunity to develop my machine learning skills and apply them to a real-world problem faced by an industry-leading company like Amazon.\n"
      ],
      "metadata": {
        "id": "oxfW6pdMfbhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#libraries and dataset"
      ],
      "metadata": {
        "id": "z-J2ybfSLgf6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx4GXWBJRAVU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-abgVIkpRI0O",
        "outputId": "0751ecf7-1f65-4e7a-fd15-9e7df7fb3d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-23 19:02:25--  https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip\n",
            "Resolving s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)... 52.219.132.10\n",
            "Connecting to s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)|52.219.132.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 895569552 (854M) [binary/octet-stream]\n",
            "Saving to: ‘datasetb2d9982.zip.1’\n",
            "\n",
            "datasetb2d9982.zip. 100%[===================>] 854.08M  18.7MB/s    in 48s     \n",
            "\n",
            "2023-04-23 19:03:14 (17.8 MB/s) - ‘datasetb2d9982.zip.1’ saved [895569552/895569552]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut65tGvnROpB",
        "outputId": "c8c60699-c215-49a0-83c8-d7e77c974afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  datasetb2d9982.zip.1\n",
            "   creating: dataset/\n",
            "  inflating: dataset/sample_submission.csv  \n",
            "  inflating: dataset/train.csv       \n",
            "  inflating: dataset/test.csv        \n"
          ]
        }
      ],
      "source": [
        "!unzip datasetb2d9982.zip.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN0JNVBYRYFw"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('dataset/train.csv')\n",
        "test_df = pd.read_csv('dataset/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tufBesiWbctv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN3CrqIXGdLw"
      },
      "source": [
        "#understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emeHDAN0GfTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abc4c4f-95c0-4261-cefa-f858252874c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2249698, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYJSOBXnGyRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2139fb6-6227-4bb5-fd32-8b9fef544257"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(734736, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9xvA2_dHD9u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "21d2fc70-c0a5-40d8-9828-f724927d940b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PRODUCT_ID                                              TITLE  \\\n",
              "0     1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
              "1     2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
              "2     2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
              "3     1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
              "4      283658  The United Empire Loyalists: A Chronicle of th...   \n",
              "\n",
              "                                       BULLET_POINTS  \\\n",
              "0  [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
              "1  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
              "2  [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
              "3  [Made By 95%cotton and 5% Lycra which gives yo...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                         DESCRIPTION  PRODUCT_TYPE_ID  \\\n",
              "0                                                NaN             1650   \n",
              "1                                                NaN             2755   \n",
              "2  Specifications: Color: Red, Material: Aluminiu...             7537   \n",
              "3  AISHAH Women's Lycra Cotton Ankel Leggings. Br...             2996   \n",
              "4                                                NaN             6112   \n",
              "\n",
              "   PRODUCT_LENGTH  \n",
              "0     2125.980000  \n",
              "1      393.700000  \n",
              "2      748.031495  \n",
              "3      787.401574  \n",
              "4      598.424000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8c7dd81-ab7f-44f7-87e7-570c09edd7e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>BULLET_POINTS</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>PRODUCT_TYPE_ID</th>\n",
              "      <th>PRODUCT_LENGTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1925202</td>\n",
              "      <td>ArtzFolio Tulip Flowers Blackout Curtain for D...</td>\n",
              "      <td>[LUXURIOUS &amp; APPEALING: Beautiful custom-made ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1650</td>\n",
              "      <td>2125.980000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2673191</td>\n",
              "      <td>Marks &amp; Spencer Girls' Pyjama Sets T86_2561C_N...</td>\n",
              "      <td>[Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2755</td>\n",
              "      <td>393.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2765088</td>\n",
              "      <td>PRIKNIK Horn Red Electric Air Horn Compressor ...</td>\n",
              "      <td>[Loud Dual Tone Trumpet Horn, Compatible With ...</td>\n",
              "      <td>Specifications: Color: Red, Material: Aluminiu...</td>\n",
              "      <td>7537</td>\n",
              "      <td>748.031495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1594019</td>\n",
              "      <td>ALISHAH Women's Cotton Ankle Length Leggings C...</td>\n",
              "      <td>[Made By 95%cotton and 5% Lycra which gives yo...</td>\n",
              "      <td>AISHAH Women's Lycra Cotton Ankel Leggings. Br...</td>\n",
              "      <td>2996</td>\n",
              "      <td>787.401574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>283658</td>\n",
              "      <td>The United Empire Loyalists: A Chronicle of th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6112</td>\n",
              "      <td>598.424000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8c7dd81-ab7f-44f7-87e7-570c09edd7e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a8c7dd81-ab7f-44f7-87e7-570c09edd7e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a8c7dd81-ab7f-44f7-87e7-570c09edd7e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V8MiQnhHyWb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "4034fabf-bf11-460e-fadb-5700f30716c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PRODUCT_ID                                              TITLE  \\\n",
              "0      604373  Manuel d'Héliogravure Et de Photogravure En Re...   \n",
              "1     1729783  DCGARING Microfiber Throw Blanket Warm Fuzzy P...   \n",
              "2     1871949  I-Match Auto Parts Front License Plate Bracket...   \n",
              "3     1107571  PinMart Gold Plated Excellence in Service 1 Ye...   \n",
              "4      624253  Visual Mathematics, Illustrated by the TI-92 a...   \n",
              "\n",
              "                                       BULLET_POINTS  \\\n",
              "0                                                NaN   \n",
              "1  [QUALITY GUARANTEED: Luxury cozy plush polyest...   \n",
              "2  [Front License Plate Bracket Made Of Plastic,D...   \n",
              "3  [Available as a single item or bulk packed. Se...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                         DESCRIPTION  PRODUCT_TYPE_ID  \n",
              "0                                                NaN             6142  \n",
              "1  <b>DCGARING Throw Blanket</b><br><br> <b>Size ...             1622  \n",
              "2  Replacement for The Following Vehicles:2020 LE...             7540  \n",
              "3  Our Excellence in Service Lapel Pins feature a...            12442  \n",
              "4                                                NaN             6318  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5af095d3-6caa-41f8-9698-e8c6e3206dad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>BULLET_POINTS</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>PRODUCT_TYPE_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>604373</td>\n",
              "      <td>Manuel d'Héliogravure Et de Photogravure En Re...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1729783</td>\n",
              "      <td>DCGARING Microfiber Throw Blanket Warm Fuzzy P...</td>\n",
              "      <td>[QUALITY GUARANTEED: Luxury cozy plush polyest...</td>\n",
              "      <td>&lt;b&gt;DCGARING Throw Blanket&lt;/b&gt;&lt;br&gt;&lt;br&gt; &lt;b&gt;Size ...</td>\n",
              "      <td>1622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1871949</td>\n",
              "      <td>I-Match Auto Parts Front License Plate Bracket...</td>\n",
              "      <td>[Front License Plate Bracket Made Of Plastic,D...</td>\n",
              "      <td>Replacement for The Following Vehicles:2020 LE...</td>\n",
              "      <td>7540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1107571</td>\n",
              "      <td>PinMart Gold Plated Excellence in Service 1 Ye...</td>\n",
              "      <td>[Available as a single item or bulk packed. Se...</td>\n",
              "      <td>Our Excellence in Service Lapel Pins feature a...</td>\n",
              "      <td>12442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>624253</td>\n",
              "      <td>Visual Mathematics, Illustrated by the TI-92 a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6318</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5af095d3-6caa-41f8-9698-e8c6e3206dad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5af095d3-6caa-41f8-9698-e8c6e3206dad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5af095d3-6caa-41f8-9698-e8c6e3206dad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "test_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulRdAiougcwI"
      },
      "source": [
        "#steps performed everytime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoH6tbUrHf-7"
      },
      "outputs": [],
      "source": [
        "##imp step\n",
        "train_df = train_df.sample(n=20000, random_state=123, replace = True)\n",
        "#test_df = test_df.sample(n=20000, random_state=123, replace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bercvnJFuIM9"
      },
      "outputs": [],
      "source": [
        "train_df = train_df[train_df['PRODUCT_LENGTH'] >= 0]\n",
        "train_df['PRODUCT_LENGTH'] = np.log1p(train_df['PRODUCT_LENGTH'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juctCKzeRtMj"
      },
      "outputs": [],
      "source": [
        "#Handle missing values\n",
        "train_df.fillna(\"unknown\", inplace=True)\n",
        "#test_df.fillna(\"unknown\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi_E7l3aRfhF"
      },
      "outputs": [],
      "source": [
        "#Combine relevant text data ('TITLE', 'DESCRIPTION', 'BULLET_POINTS') from train and test data\n",
        "combined_text = train_df['TITLE'] + \" \" + train_df['DESCRIPTION'] + \" \" + train_df['BULLET_POINTS']\n",
        "#combined_text_test = test_df['TITLE'] + \" \" + test_df['DESCRIPTION'] + \" \" + test_df['BULLET_POINTS']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#word2vec"
      ],
      "metadata": {
        "id": "MQqIERYm_LdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text) for text in combined_text_train]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(tokenized_text, min_count=1)\n",
        "\n",
        "# Create an empty numpy array to hold the word vectors\n",
        "word_vectors = np.zeros((len(tokenized_text), model.vector_size))\n",
        "\n",
        "# Convert each document to a vector representation using the trained Word2Vec model\n",
        "for i, doc in enumerate(tokenized_text):\n",
        "    vector = np.zeros(model.vector_size)\n",
        "    num_words = 0\n",
        "    for word in doc:\n",
        "        if word in model.wv:\n",
        "            vector += model.wv[word]\n",
        "            num_words += 1\n",
        "    if num_words > 0:\n",
        "        vector /= num_words\n",
        "    word_vectors[i] = vector\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(0.8 * len(word_vectors))\n",
        "x_train = word_vectors[:train_size]\n",
        "x_test = word_vectors[train_size:]\n",
        "y_train = train_df['PRODUCT_LENGTH'][:train_size]\n",
        "y_test = train_df['PRODUCT_LENGTH'][train_size:]\n",
        "\n",
        "# Train a linear regression model on the training data\n",
        "model = LinearRegression()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Calculate the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(x_test)\n",
        "accuracy = model.score(x_test, y_test)\n",
        "\n",
        "\n",
        "X_train = x_train\n",
        "X_test = x_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "elIWol8A_PsL",
        "outputId": "d6cd2a13-050b-4bef-e534-4985b3b05675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6ce576c5f953>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombined_text_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Train the Word2Vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'combined_text_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy)"
      ],
      "metadata": {
        "id": "XuzM1RitF5xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#doc2vec"
      ],
      "metadata": {
        "id": "nZxeGDvT5Qrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the combined text and create tagged documents\n",
        "tagged_docs = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[i]) for i, doc in enumerate(combined_text_train)]\n",
        "\n",
        "# Train the doc2vec model\n",
        "model = Doc2Vec(tagged_docs, vector_size=100, window=5, min_count=5, epochs=20)\n",
        "\n",
        "# Get document vectors for the training data\n",
        "doc_vectors_train = [model.infer_vector(doc.words) for doc in tagged_docs]\n"
      ],
      "metadata": {
        "id": "zX37W5Xn6C8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = [nltk.word_tokenize(text) for text in combined_text_train]\n"
      ],
      "metadata": {
        "id": "atCvaMVD5tv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extract word vectors for all words in the training data\n",
        "word_vectors_train = []\n",
        "for sentence in tokenized_text:\n",
        "    sentence_vectors = []\n",
        "    for word in sentence:\n",
        "        try:\n",
        "            word_vector = model.wv[word]\n",
        "            sentence_vectors.append(word_vector)\n",
        "        except KeyError:\n",
        "            # Ignore out-of-vocabulary words\n",
        "            pass\n",
        "    word_vectors_train.append(sentence_vectors)\n",
        "\n",
        "# Pad or truncate all sentences to a fixed length\n",
        "max_length = 50\n",
        "x_train = np.zeros((len(word_vectors_train), max_length, model.vector_size))\n",
        "for i, sentence_vectors in enumerate(word_vectors_train):\n",
        "    padded_vectors = np.zeros((max_length, model.vector_size))\n",
        "    padded_vectors[:len(sentence_vectors), :] = sentence_vectors[:max_length]\n",
        "    x_train[i, :, :] = padded_vectors\n"
      ],
      "metadata": {
        "id": "ToxUM5bw5T8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = doc_vectors_train"
      ],
      "metadata": {
        "id": "NhjYKcbA6p1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_df['PRODUCT_LENGTH'].values"
      ],
      "metadata": {
        "id": "Nskf-QmM5hPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw3A9vSmjbUx"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Tokenize and lemmatize the text\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    text = \" \".join([lemmatizer.lemmatize(token) for token in tokens])\n",
        "    \n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_xOsfz8SpO1"
      },
      "outputs": [],
      "source": [
        "#Define a function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Tokenize and stem the text\n",
        "    ps = PorterStemmer()\n",
        "    tokens = word_tokenize(text)\n",
        "    text = \" \".join([ps.stem(token) for token in tokens])\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78Lms32bUqKg"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mabcyAnU0Sq"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcB9WevWTCM3"
      },
      "outputs": [],
      "source": [
        "#Preprocess the combined text data\n",
        "preprocessed_text_train = combined_text_train.apply(preprocess_text)\n",
        "#preprocessed_text_test = combined_text_test.apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm59ELUzdJGc"
      },
      "outputs": [],
      "source": [
        "#Vectorize the preprocessed text using TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_text = vectorizer.fit_transform(preprocessed_text_train)\n",
        "#X_test_text = vectorizer.fit_transform(preprocessed_text_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Mjwxo-LxH-"
      },
      "outputs": [],
      "source": [
        "X_train = X_train_text.toarray()\n",
        "#X_test_text_array = X_test_text.toarray()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqmIMnJgeBBw"
      },
      "outputs": [],
      "source": [
        "type(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYjvShefd_Ml"
      },
      "outputs": [],
      "source": [
        "y_train = train_df['PRODUCT_LENGTH'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KL0cKtYeE5t"
      },
      "outputs": [],
      "source": [
        "type(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ie46ni1O94_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming your input array is called 'input_array'\n",
        "input_array_flattened = np.flatten(X_train_text_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHyb39CVOaLD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "product_type_id_encoded = encoder.fit_transform(train_df['PRODUCT_TYPE_ID'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMA7DBu2RNhm"
      },
      "outputs": [],
      "source": [
        "product_type_id_encoded_2d = product_type_id_encoded.reshape(-1, 1)\n",
        "print(product_type_id_encoded_2d.shape)\n",
        "print(X_train_text_array.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7t8hJPmQy62"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "# Concatenate X_train_text_array and product_type_id_encoded\n",
        "X_train = hstack((product_type_id_encoded.reshape(-1, 1),X_train_text_array.T ))\n",
        "\n",
        "# Convert X_train to a dense numpy array\n",
        "X_train = X_train.toarray()\n",
        "\n",
        "# Reshape y_train to a 2D numpy array\n",
        "y_train = train_df['PRODUCT_LENGTH'].values.reshape(-1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo_qwo1dQDO_"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "# Concatenate X_train_text_array and product_type_id_encoded\n",
        "X_train_encoded = hstack((X_train_text_array, product_type_id_encoded.reshape(-1, 1)))\n",
        "\n",
        "# Convert X_train_encoded to a dense numpy array\n",
        "X_train = X_train_encoded.toarray()\n",
        "\n",
        "y_train = train_df['PRODUCT_LENGTH'].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLByjXwkNHKA"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "# Concatenate X_train_text_array and product_type_id_encoded\n",
        "X_train = hstack((X_train_text_array, product_type_id_encoded.reshape(-1, 1)))\n",
        "\n",
        "# Convert X_train to a dense numpy array\n",
        "X_train = X_train.toarray()\n",
        "y_train = train_data['PRODUCT_LENGTH'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FquSpC5uEhwn"
      },
      "outputs": [],
      "source": [
        "# Prepare the input data for the machine learning model\n",
        "X_train = pd.concat([pd.DataFrame(X_train_text.toarray()), train_data['PRODUCT_TYPE_ID'].reset_index(drop=True)], axis=1)\n",
        "X_val = pd.concat([pd.DataFrame(X_val_text.toarray()), validation_data['PRODUCT_TYPE_ID'].reset_index(drop=True)], axis=1)\n",
        "y_train = train_data['PRODUCT_LENGTH']\n",
        "y_val = validation_data['PRODUCT_LENGTH']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAhIqpghLqsi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfeM5PQoKcBY"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWfqffsJKgqA"
      },
      "outputs": [],
      "source": [
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLRWTXeOKj99"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = model.score(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuZaDTW0gvZJ"
      },
      "source": [
        "#TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set = train_test_split(train_df, test_size=0.2, random_state=123)"
      ],
      "metadata": {
        "id": "9ineyO7dSLiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "train_features = vectorizer.fit_transform(train_set['TITLE'].fillna('') + ' ' + train_set['DESCRIPTION'].fillna('') + ' ' + train_set['BULLET_POINTS'].fillna(''))\n",
        "val_features = vectorizer.transform(val_set['TITLE'].fillna('') + ' ' + val_set['DESCRIPTION'].fillna('') + ' ' + val_set['BULLET_POINTS'].fillna(''))"
      ],
      "metadata": {
        "id": "bnxyxG2pSZwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define target variable\n",
        "target_col = 'PRODUCT_LENGTH'\n",
        "\n",
        "# extract x_train and y_train\n",
        "X_train = train_features\n",
        "y_train = train_set[target_col]\n",
        "\n",
        "# extract x_test and y_test\n",
        "X_test = val_features\n",
        "y_test = val_set[target_col]"
      ],
      "metadata": {
        "id": "9zLgVgbSS0Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tf idf try"
      ],
      "metadata": {
        "id": "v4Q7wyL2TGM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the combined_text column using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(combined_text)\n",
        "X_test = vectorizer.transform(combined_text)"
      ],
      "metadata": {
        "id": "UaSlICobPLg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.toarray()\n",
        "X_test.toarray()"
      ],
      "metadata": {
        "id": "l0rFqRFBQwRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c5l5DJSZLOR"
      },
      "outputs": [],
      "source": [
        "#first split the data into training and validation sets and then develop a machine learning model to predict the product length dimension. \n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the training data\n",
        "data = pd.read_csv('dataset/preprocessed_train.csv')\n",
        "\n",
        "##imp step\n",
        "data = data.sample(n=20000, random_state=123, replace = True)\n",
        "\n",
        "# Preprocess the data: combine TITLE, DESCRIPTION, and BULLET_POINTS into a single text feature\n",
        "data['combined_text'] = data['TITLE'].fillna('') + ' ' + data['DESCRIPTION'].fillna('') + ' ' + data['BULLET_POINTS'].fillna('')\n",
        "data = data[['combined_text', 'PRODUCT_TYPE_ID', 'PRODUCT_LENGTH']]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, validation_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the combined_text column using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_text = vectorizer.fit_transform(train_data['combined_text'])\n",
        "X_val_text = vectorizer.transform(validation_data['combined_text'])\n",
        "\n",
        "# Prepare the input data for the machine learning model\n",
        "X_train = pd.concat([pd.DataFrame(X_train_text.toarray()), train_data['PRODUCT_TYPE_ID'].reset_index(drop=True)], axis=1)\n",
        "X_val = pd.concat([pd.DataFrame(X_val_text.toarray()), validation_data['PRODUCT_TYPE_ID'].reset_index(drop=True)], axis=1)\n",
        "y_train = train_data['PRODUCT_LENGTH']\n",
        "y_val = validation_data['PRODUCT_LENGTH']\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Validate the model on the validation set\n",
        "y_pred_val = model.predict(X_val)\n",
        "\n",
        "#calculate accuracy\n",
        "accuracy = model.score(X_val, y_val)\n",
        "print(\"accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Concatenation skip"
      ],
      "metadata": {
        "id": "x81AnKCbLPJy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klwfDb0RTcgF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Extract the vectors from column A into a numpy array\n",
        "X_a = np.stack(df['A'].values)\n",
        "\n",
        "# Extract the integer values from column B into a numpy array\n",
        "X_b = df['B'].values.reshape(-1, 1)\n",
        "\n",
        "# Concatenate the two numpy arrays along axis 1 to create the final feature matrix\n",
        "X_train = np.concatenate([X_a, X_b], axis=1)\n",
        "\n",
        "# Extract the target variable into a numpy array\n",
        "y_train = df['C'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxEDbCBZ4f3v"
      },
      "source": [
        "#new start skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7MWlhFF4e7R"
      },
      "outputs": [],
      "source": [
        "#To preprocess the dataset efficiently and ensure the code runs on Google Colab without crashing, follow the steps below:\n",
        "\n",
        "#1. Download the dataset.\n",
        "#2. Load train.csv and test.csv files.\n",
        "#3. Handle missing values.\n",
        "#4. Remove noise.\n",
        "#5. Normalize the data.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download and unzip the dataset\n",
        "#!wget https://s3-ap-southeast-1.amazonaws.com/he-public-data/datasetb2d9982.zip\n",
        "#!unzip datasetb2d9982.zip\n",
        "\n",
        "# Load the train.csv and test.csv files\n",
        "train_data = pd.read_csv(\"dataset/train.csv\", index_col=\"PRODUCT_ID\")\n",
        "test_data = pd.read_csv(\"dataset/test.csv\", index_col=\"PRODUCT_ID\")\n",
        "\n",
        "# Combine the text columns into a single column\n",
        "train_data[\"combined_text\"] = train_data[\"TITLE\"].fillna('') + ' ' + train_data[\"DESCRIPTION\"].fillna('') + ' ' + train_data[\"BULLET_POINTS\"].fillna('')\n",
        "test_data[\"combined_text\"] = test_data[\"TITLE\"].fillna('') + ' ' + test_data[\"DESCRIPTION\"].fillna('') + ' ' + test_data[\"BULLET_POINTS\"].fillna('')\n",
        "\n",
        "# Drop the individual text columns\n",
        "train_data.drop([\"TITLE\", \"DESCRIPTION\", \"BULLET_POINTS\"], axis=1, inplace=True)\n",
        "test_data.drop([\"TITLE\", \"DESCRIPTION\", \"BULLET_POINTS\"], axis=1, inplace=True)\n",
        "\n",
        "# Fill missing values in product_type_id using SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "train_data[\"PRODUCT_TYPE_ID\"] = imputer.fit_transform(train_data[\"PRODUCT_TYPE_ID\"].values.reshape(-1, 1))\n",
        "test_data[\"PRODUCT_TYPE_ID\"] = imputer.fit_transform(test_data[\"PRODUCT_TYPE_ID\"].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY3hDbkQOb_i"
      },
      "outputs": [],
      "source": [
        "#droping missing values \n",
        "train_df.dropna(inplace=True)\n",
        "print(train_df.isnull().sum())\n",
        "train_df.info()\n",
        "unique_product_df = train_df.TITLE.unique().tolist()\n",
        "print(len(unique_product_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZoRCRUZQEjW"
      },
      "source": [
        "#run for now (vectorization for title, description, bullet **separately**) skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtY9Mzoqg3zj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kXV2Pkj5DRN"
      },
      "outputs": [],
      "source": [
        "#To perform feature engineering on the given dataset, we will do the following steps:\n",
        "\n",
        "#1. Import necessary libraries\n",
        "#2. Load the training dataset\n",
        "#3. Preprocess text data\n",
        "#4. Vectorize text data using TF-IDF\n",
        "#5. Encode categorical data\n",
        "#6. Combine features\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataa = pd.read_csv('dataset/preprocessed_train.csv')\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    return text.str.lower().str.replace('[^a-z\\s]', '')\n",
        "\n",
        "train_dataa['TITLE'] = preprocess_text(train_dataa['TITLE'])\n",
        "train_dataa['DESCRIPTION'] = preprocess_text(train_dataa['DESCRIPTION'])\n",
        "train_dataa['BULLET_POINTS'] = preprocess_text(train_dataa['BULLET_POINTS'])\n",
        "\n",
        "# Vectorize text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "title_matrix = vectorizer.fit_transform(train_dataa['TITLE'])\n",
        "description_matrix = vectorizer.fit_transform(train_dataa['DESCRIPTION'])\n",
        "bullet_points_matrix = vectorizer.fit_transform(train_dataa['BULLET_POINTS'])\n",
        "\n",
        "# Encode categorical data\n",
        "encoder = LabelEncoder()\n",
        "product_type_id_encoded = encoder.fit_transform(train_dataa['PRODUCT_TYPE_ID'])\n",
        "\n",
        "# Combine features\n",
        "#X_train = hstack((title_matrix, description_matrix, bullet_points_matrix, product_type_id_encoded.reshape(-1, 1)))\n",
        "X_train = pd.DataFrame(title_matrix, description_matrix, bullet_points_matrix, product_type_id_encoded.reshape(-1, 1))\n",
        "y_train = pd.DataFrame(train_dataa['PRODUCT_LENGTH'])\n",
        "#y_train = train_dataa['PRODUCT_LENGTH'].values\n",
        "\n",
        "print(\"Features have been engineered.\")\n",
        "\n",
        "type(X)\n",
        "type(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5beAidFhgY7"
      },
      "source": [
        "#Word2Vec (feature matrix include product_type_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gcQTC17yZph"
      },
      "outputs": [],
      "source": [
        "#info\n",
        "#a = np.array([[1, 2], [3, 4]])\n",
        "#b = np.array([[5, 6], [7,8]])\n",
        "#np.concatenate((a, b), axis=0)#along row\n",
        "#np.concatenate((a, b), axis=1)#along column #horizontal stacking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWI5tjLhhRDL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXcKq6sHhU9s"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('dataset/preprocessed_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7T-kFXs-W5I"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygwhIVb75yOl"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.sample(n=20000, random_state=123, replace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex3TEpBQhmIY"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    return text.str.lower().str.replace('[^a-z\\s]', '')\n",
        "\n",
        "train_data['TITLE'] = preprocess_text(train_data['TITLE'])\n",
        "train_data['DESCRIPTION'] = preprocess_text(train_data['DESCRIPTION'])\n",
        "train_data['BULLET_POINTS'] = preprocess_text(train_data['BULLET_POINTS'])\n",
        "\n",
        "print(\"punctuation removal and lower case conversion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eFj3ZTWhxxa"
      },
      "outputs": [],
      "source": [
        "# Get the first half of the data\n",
        "#half_data = train_data.iloc[:len(train_data)//2]\n",
        "\n",
        "# Create a list of all the sentences in the selected columns\n",
        "sentences = train_data['TITLE'].apply(lambda x: x.split()).tolist() + train_data['DESCRIPTION'].apply(lambda x: x.split()).tolist() + train_data['BULLET_POINTS'].apply(lambda x: x.split()).tolist()\n",
        "\n",
        "# Train a Word2Vec model on the selected sentences\n",
        "model = Word2Vec(sentences, min_count=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuU_xkaC67oU"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD9HUGnxBJnw"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "product_type_id_encoded = encoder.fit_transform(train_data['PRODUCT_TYPE_ID'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ean2HZprBM5p"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "for i in range(train_df.shape[0]):\n",
        "    title = train_df.iloc[i]['TITLE'].split()\n",
        "    description = train_df.iloc[i]['DESCRIPTION'].split()\n",
        "    bullet_points = train_df.iloc[i]['BULLET_POINTS'].split()\n",
        "    features = [model.wv[word] for word in title + description + bullet_points]\n",
        "    features = np.array(features).mean(axis=0)\n",
        "    #X_train.append(np.concatenate((features, [product_type_id_encoded[i]])))\n",
        "    X_train.append(features)\n",
        "X_train = np.array(X_train)\n",
        "y_train = train_data['PRODUCT_LENGTH'].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exbLLYT5iCeK"
      },
      "source": [
        "#save & load pickle file skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgvHlsTSSQkS"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save X and y as pickle files\n",
        "with open('X.pickle', 'wb') as f:\n",
        "    pickle.dump(X, f)\n",
        "\n",
        "with open('y.pickle', 'wb') as f:\n",
        "    pickle.dump(y, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPo6Tu6nST17"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load X and y from pickle files\n",
        "with open('X.pickle', 'rb') as f:\n",
        "    X = pickle.load(f)\n",
        "\n",
        "with open('y.pickle', 'rb') as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaiL1lJxKC-j"
      },
      "source": [
        "#this is working [1st pre processing] (doesnt yield any special result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maQ7E6Ob-kE1"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('dataset/train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcMTGDmO-1VT"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yDBlAn7-lHA"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85KDJQX0_ANr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Load the training data\n",
        "train_df = pd.read_csv('dataset/train.csv')\n",
        "\n",
        "# Remove duplicates based on TITLE\n",
        "train_df.drop_duplicates(subset='TITLE', keep='first', inplace=True)\n",
        "\n",
        "# Fill missing values with 'unknown'\n",
        "train_df.fillna(\"unknown\", inplace=True)\n",
        "\n",
        "# Remove special characters and convert to lowercase\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Combine TITLE, DESCRIPTION, and BULLET_POINTS into a single text feature\n",
        "train_df['text'] = train_df['TITLE'] + ' ' + train_df['DESCRIPTION'] + ' ' + train_df['BULLET_POINTS']\n",
        "\n",
        "# Preprocess the text feature\n",
        "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
        "\n",
        "# Save the preprocessed data to a new CSV file\n",
        "train_df.to_csv('dataset/preprocessed_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GrxQpod_srS"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utPgBiUW_wUS"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvg4gAHSDzNQ"
      },
      "outputs": [],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czlneJstQzB0"
      },
      "source": [
        "#this part is crashing skip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRSSteFpDGJz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Define the TF-IDF vectorizer\n",
        "#vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# Define the Count vectorizer\n",
        "vectorizer = CountVectorizer(max_features=10000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_text = vectorizer.fit_transform(train_df['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzD4ubdEJaBN"
      },
      "source": [
        "next part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uroxswkImSA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# Process the data in chunks\n",
        "X_train_list = []\n",
        "for chunk in pd.read_csv('train.csv', chunksize=10000):\n",
        "    # Preprocess the data\n",
        "    chunk.fillna(\"unknown\", inplace=True)\n",
        "\n",
        "    # Fit and transform the chunk\n",
        "    X_chunk = vectorizer.fit_transform(chunk['text'])\n",
        "    X_train_list.append(X_chunk)\n",
        "\n",
        "# Concatenate the transformed chunks into a single sparse matrix\n",
        "X_train = scipy.sparse.vstack(X_train_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl9baBVCF5x7"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(stopwords=,max_features=10000)\n",
        "\n",
        "# Split the data into smaller chunks\n",
        "chunk_size = 10000\n",
        "chunks = [train_df['text'][i:i+chunk_size] for i in range(0, len(train_df), chunk_size)]\n",
        "\n",
        "# Fit the vectorizer on the first chunk\n",
        "vectorizer.partial_fit(chunks[0])\n",
        "\n",
        "# Transform the data in chunks\n",
        "for chunk in chunks[1:]:\n",
        "    X_chunk = vectorizer.transform(chunk)\n",
        "    X_train = vstack([X_train, X_chunk])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXPBhhakQ34m"
      },
      "source": [
        "#working coz of chunks, but crashes sometimes too "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br6qphVTMnod"
      },
      "outputs": [],
      "source": [
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R3MJqHBJq6V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# Process the data in chunks\n",
        "X_train_list = []\n",
        "i=0\n",
        "for chunk in pd.read_csv('dataset/preprocessed_train.csv', chunksize=10000):\n",
        "    \n",
        "    print(i)\n",
        "    i= i+1\n",
        "    # Fit and transform the chunk\n",
        "    X_chunk = vectorizer.fit_transform(chunk['text'])\n",
        "    X_train_list.append(X_chunk)\n",
        "    print(i)\n",
        "\n",
        "import scipy\n",
        "from scipy.sparse import vstack\n",
        "# Concatenate the transformed chunks into a single sparse matrix\n",
        "X_train_text = scipy.sparse.vstack(X_train_list)\n",
        "\n",
        "# Save the transformed data\n",
        "X_train_text = pd.DataFrame(X_train_text.toarray())\n",
        "X_train_text.to_csv('dataset/X_train_text.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxPnSVjnkrIl"
      },
      "outputs": [],
      "source": [
        "X_train_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyT5Obk5krUA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdLTFE_GRSxb"
      },
      "outputs": [],
      "source": [
        "# Save the transformed data\n",
        "X_train_text = pd.DataFrame(X_train_text.toarray())\n",
        "X_train_text.to_csv('X_train_text.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEeK68QcRGEB"
      },
      "source": [
        "# linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCV7wLWnM2SC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "X_train = pd.concat([pd.DataFrame(X.toarray()), train_df['PRODUCT_TYPE_ID'].reset_index(drop=True)], axis=1)\n",
        "y_train = train_df['PRODUCT_LENGTH']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-oi2v3IX5dF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(combined_text, y_train, test_size=0.7, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtONV9DEM5VD"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXK0lUepYRu-"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = model.score(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNG42kgyavWn"
      },
      "outputs": [],
      "source": [
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EjFgLoKXT80"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNNR7xNGe75e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('R-squared score:', r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHHWmFR9D8jm"
      },
      "source": [
        "#decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poJK0pgnD-Tm"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Train a decision tree regression model\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#calculate accuracy\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"accuracy:\", accuracy)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kivzyo1OfyLa"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('R-squared score:', r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESGI7DE9wLv3"
      },
      "source": [
        "#linear svr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCeWmN0rwL_t"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "# initialize the model\n",
        "model = LinearSVR()\n",
        "\n",
        "# fit the model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(accuracy)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5UuTj01wmvO"
      },
      "source": [
        "#gradient booster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC4dekZtwqKd"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# initialize the model\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
        "\n",
        "# fit the model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(accuracy)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3knd4wapxxsR"
      },
      "source": [
        "#random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3Nh3WnXxv_K"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the model\n",
        "model = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
        "\n",
        "# Fit the model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"accuracy: \", accuracy)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dNlnXPnyQ-W"
      },
      "source": [
        "#XG Booster regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk5L246FyP6p"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create XGBRegressor object with hyperparameters\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    max_depth=3, \n",
        "    learning_rate=0.1, \n",
        "    n_estimators=1200, \n",
        "    objective='reg:squarederror')\n",
        "\n",
        "# Train the mo  del\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"accuracy: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the sequence length and number of features\n",
        "sequence_length = 100\n",
        "num_features = 10000\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=num_features, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(combined_text)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(combined_text)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=sequence_length, truncating='post')\n",
        "\n",
        "# Create an RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_features, output_dim=64, input_length=sequence_length))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n"
      ],
      "metadata": {
        "id": "AVNNxPgLIcKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}